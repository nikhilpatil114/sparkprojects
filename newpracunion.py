import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark=SparkSession.builder.master("local[*]").appName("myapp").getOrCreate()
data="E:\\bigdatafile\\ds1.csv"
data1="E:\\bigdatafile\\ds2.csv"
df1=spark.read.format('csv').option("header","true").option('inferSchema','true').load(data)
df2=spark.read.format('csv').option("header","true").option('inferSchema','true').load(data1)
df1.agg({'sal':'sum'}).show()
#df2.show()
#df1.union(df2).distinct().show()
#df1.dropDuplicates(subset=["name","sal"]).show()
#df1.toDF()
"""
df1.unionAll()
df1.join()
df1.show()
df1.select()
df1.union()
df1.distinct()
df1.dropDuplicates()
df1.drop()
df1.orderBy()
df1.filter()
df1.withColumn()
df1.columns()
df1.createOrReplaceTempView()
df1.collect()
df1.groupBy()
df1.alias()
df1.head()
df1.sort()
df1.coalesce()
df1.repartition()
df1.persist()
df1.toPandas()
df1.agg({'sal':'sum'}).show()
df1.cache()
df1.withColumnRenamed()
df1.dropna()
df1.fillna()
df1.tail()
df1.where()
df1.createGlobalTempView()
df1.createTempView()
"""